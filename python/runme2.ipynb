{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nonlinear Classification\n",
    "Under construction (3 Oct 2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "dtype = torch.float\n",
    "import NeuralNetwork as NN\n",
    "import Layer\n",
    "import importlib\n",
    "import time\n",
    "from copy import deepcopy\n",
    "from IPython.display import display\n",
    "from ipywidgets import FloatProgress\n",
    "\n",
    "importlib.reload(NN)\n",
    "importlib.reload(Layer);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def MakeBatches(data_in, data_out, batch_size=10):\n",
    "    '''\n",
    "    batches = MakeBatches(data_in, data_out, batch_size=10)\n",
    "    \n",
    "    Breaks up the dataset into batches of size batch_size.\n",
    "    \n",
    "    Inputs:\n",
    "      data_in    is a list of inputs\n",
    "      data_out   is a list of outputs\n",
    "      batch_size is the number of samples in each batch\n",
    "      \n",
    "    Output:\n",
    "      batches is a list containing batches, where each batch is:\n",
    "                 [in_batch, out_batch]\n",
    "    '''\n",
    "    N = len(data_in)\n",
    "    batches = []\n",
    "    for k in range(0, N, batch_size):\n",
    "        din = data_in[k:k+batch_size]\n",
    "        dout = data_out[k:k+batch_size]\n",
    "        if isinstance(din, (list, tuple)):\n",
    "            batches.append( [torch.stack(din, dim=0).float().to(device) , torch.stack(dout, dim=0).float().to(device)] )\n",
    "        else:\n",
    "            batches.append( [din.float().to(device) , dout.float().to(device)] )\n",
    "    return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\") # Uncomment this to run on GPU\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary one-hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1 -1  1 -1 -1  1  1 -1] -> [1. 0. 0. 0. 0.]\n",
      "[-1  1 -1  1 -1  1 -1  1] -> [0. 1. 0. 0. 0.]\n",
      "[-1  1  1 -1  1 -1 -1  1] -> [0. 0. 1. 0. 0.]\n",
      "[ 1 -1 -1 -1  1 -1  1  1] -> [0. 0. 0. 1. 0.]\n",
      "[ 1 -1 -1  1 -1  1 -1  1] -> [0. 0. 0. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "# 8->5 binary one-hot\n",
    "noise = 0.\n",
    "n_samples = 100\n",
    "s = np.array([[1,0,1,0,0,1,1,0],\n",
    "                         [0,1,0,1,0,1,0,1],\n",
    "                         [0,1,1,0,1,0,0,1],\n",
    "                         [1,0,0,0,1,0,1,1],\n",
    "                         [1,0,0,1,0,1,0,1]], dtype=float)\n",
    "e = np.eye(5)\n",
    "s = s*2. - 1.\n",
    "#e = e*2. - 1.\n",
    "classes = len(s)\n",
    "training_input = []\n",
    "training_output = []\n",
    "r = 0\n",
    "for n in range(n_samples):\n",
    "    training_input.append(np.array(s[r], dtype=float) + noise*np.random.normal(scale=0.01,size=len(s[r])))\n",
    "    training_output.append(np.array(e[r], dtype=float))\n",
    "    r = np.mod(r+1, classes) #r = np.random.randint(classes)\n",
    "\n",
    "training_input = torch.tensor(training_input).float().to(device)\n",
    "training_output = torch.tensor(training_output).float().to(device)\n",
    "train = [training_input, training_output]\n",
    "\n",
    "n_test_samples = 100\n",
    "test_input = []\n",
    "test_output = []\n",
    "r = 0\n",
    "for n in range(n_test_samples):\n",
    "    test_input.append(np.array(s[r], dtype=float) + noise*np.random.normal(scale=0.01,size=len(s[r])))\n",
    "    test_output.append(np.array(e[r], dtype=float))\n",
    "    r = np.mod(r+1, classes) #np.random.randint(classes)\n",
    "\n",
    "test_input = torch.tensor(test_input).float().to(device)\n",
    "test_output = torch.tensor(test_output).float().to(device)\n",
    "test = [test_input, test_output]\n",
    "\n",
    "for x, t in zip(s,e):\n",
    "    print(str(np.array(x,dtype=int))+' -> '+str(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'NeuralNetwork' from '/Users/jorchard/Dropbox/research/peBogacz/python/NeuralNetwork.py'>"
      ]
     },
     "execution_count": 531,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(Layer)\n",
    "importlib.reload(NN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = NN.NeuralNetwork()\n",
    "net.AddLayer(Layer.InputPELayer(n=8))\n",
    "net.AddLayer(Layer.PELayer(n=7))\n",
    "net.Connect(0, 1)\n",
    "net.AddLayer(Layer.PELayer(n=6))\n",
    "net.Connect(1,2)\n",
    "net.AddLayer(Layer.TopPELayer(n=5))\n",
    "net.Connect(2,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1 -1  1 -1 -1  1  1 -1] -> [-1.  1.  1.  1.]\n",
      "[-1  1 -1  1 -1  1 -1  1] -> [ 1.  1. -1.  1.]\n",
      "[-1  1  1 -1  1 -1 -1  1] -> [ 1. -1.  1.  1.]\n",
      "[ 1 -1 -1 -1  1 -1  1  1] -> [ 1.  1.  1. -1.]\n",
      "[ 1 -1 -1  1 -1  1 -1  1] -> [-1. -1. -1. -1.]\n"
     ]
    }
   ],
   "source": [
    "# 8->4 binary mapping\n",
    "noise = 0.\n",
    "n_samples = 100\n",
    "s = np.array([[1,0,1,0,0,1,1,0],\n",
    "                         [0,1,0,1,0,1,0,1],\n",
    "                         [0,1,1,0,1,0,0,1],\n",
    "                         [1,0,0,0,1,0,1,1],\n",
    "                         [1,0,0,1,0,1,0,1]], dtype=float)\n",
    "e = np.array([[0,1,1,1],[1,1,0,1],[1,0,1,1],[1,1,1,0],[0,0,0,0]])\n",
    "s = s*2. - 1.\n",
    "e = e*2. - 1.\n",
    "classes = len(s)\n",
    "training_input = []\n",
    "training_output = []\n",
    "r = 0\n",
    "for n in range(n_samples):\n",
    "    r = np.mod(r+1, classes) #r = np.random.randint(classes)\n",
    "    training_input.append(np.array(s[r], dtype=float) + noise*np.random.normal(scale=0.01,size=len(s[r])))\n",
    "    training_output.append(np.array(e[r], dtype=float))\n",
    "\n",
    "training_input = torch.tensor(training_input).float().to(device)\n",
    "training_output = torch.tensor(training_output).float().to(device)\n",
    "train = [training_input, training_output]\n",
    "\n",
    "n_test_samples = 100\n",
    "test_input = []\n",
    "test_output = []\n",
    "r = 0\n",
    "for n in range(n_test_samples):\n",
    "    r = np.mod(r+1, classes) #np.random.randint(classes)\n",
    "    test_input.append(np.array(s[r], dtype=float) + noise*np.random.normal(scale=0.01,size=len(s[r])))\n",
    "    test_output.append(np.array(e[r], dtype=float))\n",
    "\n",
    "test_input = torch.tensor(test_input).float().to(device)\n",
    "test_output = torch.tensor(test_output).float().to(device)\n",
    "test = [test_input, test_output]\n",
    "\n",
    "for x, t in zip(s,e):\n",
    "    print(str(np.array(x,dtype=int))+' -> '+str(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net = NN.NeuralNetwork()\n",
    "net.AddLayer(Layer.InputPELayer(n=8))\n",
    "net.AddLayer(Layer.PELayer(n=7))\n",
    "net.Connect(0, 1)\n",
    "net.AddLayer(Layer.PELayer(n=6))\n",
    "net.Connect(1,2)\n",
    "net.AddLayer(Layer.TopPELayer(n=4))\n",
    "net.Connect(2,3)\n",
    "# net.ConnectNextLayer(Layer.InputPELayer(n=8))\n",
    "# net.ConnectNextLayer(Layer.PELayer(n=7))\n",
    "# net.ConnectNextLayer(Layer.PELayer(n=6))\n",
    "# net.ConnectNextLayer(Layer.TopPELayer(n=4))\n",
    "net.layers[-1].sigma = Layer.tanh\n",
    "net.layers[-1].sigma_p = Layer.tanh_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function Layer.tanh>"
      ]
     },
     "execution_count": 544,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.layers[-1].sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Polar Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "n_theta = 4\n",
    "n_rho = 3\n",
    "s = []\n",
    "e = []\n",
    "#for theta in np.linspace(2.*np.pi/(n_theta+2.), 2.*np.pi*(n_theta+1.)/(n_theta+2.), n_theta, endpoint=True):\n",
    "for theta in np.linspace(-1., 1, n_theta+1, endpoint=False):\n",
    "    if theta != -1.:\n",
    "        theta_radians = theta*np.pi\n",
    "        for rho in np.linspace(0., 1., n_rho+1, endpoint=False):\n",
    "            if rho != 0.:\n",
    "                rho2 = rho - 0.5\n",
    "                e.append([rho2, theta])\n",
    "                x = rho*np.cos(theta_radians)\n",
    "                y = rho*np.sin(theta_radians)\n",
    "                s.append([x,y])\n",
    "e = np.array(e)\n",
    "s = np.array(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "training_samples = 100\n",
    "r = 0\n",
    "training_input = []\n",
    "training_output = []\n",
    "for n in range(training_samples):\n",
    "    r = np.mod(r+1, len(s)) #r = np.random.randint(classes)\n",
    "    training_input.append(np.array(s[r,:], dtype=float) )#+ noise*np.random.normal(scale=0.01,size=len(s[r,:])))\n",
    "    training_output.append(np.array(e[r,:], dtype=float))\n",
    "\n",
    "training_input = torch.tensor(training_input).float().to(device)\n",
    "training_output = torch.tensor(training_output).float().to(device)\n",
    "train = [training_input, training_output]\n",
    "\n",
    "n_test_samples = 100\n",
    "test_input = []\n",
    "test_output = []\n",
    "r = 0\n",
    "for n in range(n_test_samples):\n",
    "    r = np.mod(r+1, len(s)) #r = np.random.randint(classes)\n",
    "    test_input.append(np.array(s[r,:], dtype=float) )#+ noise*np.random.normal(scale=0.01,size=len(s[r,:])))\n",
    "    test_output.append(np.array(e[r,:], dtype=float))\n",
    "\n",
    "training_input = torch.tensor(training_input).float().to(device)\n",
    "training_output = torch.tensor(training_output).float().to(device)\n",
    "test = [test_input, test_output]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQsAAAD8CAYAAABgtYFHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMSwkFTEsJBX5f74JFRHvZZuAAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x13f406128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(s[:,0], s[:,1], 'b.')\n",
    "plt.axis('square');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "net = NN.NeuralNetwork()\n",
    "net.AddLayer(Layer.InputPELayer(n=2))\n",
    "net.AddLayer(Layer.PELayer(n=6))\n",
    "net.AddLayer(Layer.PELayer(n=8))\n",
    "net.AddLayer(Layer.TopPELayer(n=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c2521fd6649473cac4f3c578a4daf14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time: 4.618978977203369\n"
     ]
    }
   ],
   "source": [
    "train_shuffle = list(zip(train[0],train[1]))\n",
    "net.learning_tau = 2.\n",
    "batch_size = 10\n",
    "epochs = 5\n",
    "fp = FloatProgress(min=0,max=epochs*len(train_shuffle))\n",
    "display(fp)\n",
    "T = 3\n",
    "start_time = time.time()\n",
    "for k in range(epochs):\n",
    "    np.random.shuffle(train_shuffle)\n",
    "    unzip = list(zip(*train_shuffle))\n",
    "    batches = MakeBatches(unzip[0], unzip[1], batch_size)\n",
    "    for x in batches:\n",
    "        net.Infer(T, x[0], x[1])\n",
    "        fp.value += batch_size\n",
    "end_time = time.time()\n",
    "print('Total time: '+str(end_time-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net.Save('blah.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the model on the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net.Reset()\n",
    "yy = net.Predict(10., test[0])\n",
    "y_true = test[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.9740, -0.9910, -0.9280, -0.9480],\n",
      "        [ 0.8990, -0.9600,  0.9490,  0.9210],\n",
      "        [ 0.8720,  0.9010,  0.9460, -0.9730],\n",
      "        [-0.9930, -0.9750, -0.8490, -0.9870],\n",
      "        [-0.9930,  0.9450,  0.9870,  0.7570],\n",
      "        [-0.9740, -0.9910, -0.9280, -0.9480],\n",
      "        [ 0.8990, -0.9600,  0.9490,  0.9210],\n",
      "        [ 0.8720,  0.9010,  0.9460, -0.9730],\n",
      "        [-0.9930, -0.9750, -0.8490, -0.9870],\n",
      "        [-0.9930,  0.9450,  0.9870,  0.7570]])\n",
      "tensor([[ 1.,  1., -1.,  1.],\n",
      "        [ 1., -1.,  1.,  1.],\n",
      "        [ 1.,  1.,  1., -1.],\n",
      "        [-1., -1., -1., -1.],\n",
      "        [-1.,  1.,  1.,  1.],\n",
      "        [ 1.,  1., -1.,  1.],\n",
      "        [ 1., -1.,  1.,  1.],\n",
      "        [ 1.,  1.,  1., -1.],\n",
      "        [-1., -1., -1., -1.],\n",
      "        [-1.,  1.,  1.,  1.]])\n"
     ]
    }
   ],
   "source": [
    "print(np.round(yy[:10],decimals=3))\n",
    "print(y_true[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net.Reset()\n",
    "yy = net.Generate(10., test[1])\n",
    "y_true = test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.0%\n"
     ]
    }
   ],
   "source": [
    "# Test for binary strings dataset\n",
    "fail = 0\n",
    "for x,t in zip(yy, y_true):\n",
    "    if np.isnan(t).any() or torch.min(x*t).le(0.):\n",
    "        fail += 1\n",
    "n_trials = float(len(y_true))\n",
    "print('Accuracy '+str((n_trials-fail)/n_trials*100.)+'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 1.0\n"
     ]
    }
   ],
   "source": [
    "# Test for polar clusters dataset\n",
    "fail = 0\n",
    "for x,t in zip(yy, y_true):\n",
    "    if np.isnan(t).any() or torch.max(abs(x-t)).ge(0.2):\n",
    "        fail += 1\n",
    "n_trials = float(len(test[0]))\n",
    "print('Accuracy '+str((n_trials-fail)/n_trials))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.1699,  0.1597,  0.1946,  0.3365,  0.1393])\n",
      "tensor([ 0.,  1.,  0.,  0.,  0.])\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.randint(len(test[0]))\n",
    "net.Reset()\n",
    "print(net.Predict(5., test[0][idx]))\n",
    "print(test[1][idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net2 = NN.NeuralNetwork()\n",
    "net2.Load('blah.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 7])"
      ]
     },
     "execution_count": 406,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(net2.W[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([nan., nan., nan., nan., nan.])\n",
      "tensor([ 0.,  1.,  0.,  0.,  0.])\n"
     ]
    }
   ],
   "source": [
    "net2.Reset()\n",
    "print(net2.Predict(5., test[0][idx]))\n",
    "print(test[1][idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7])\n",
      "torch.Size([7, 8])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 7])\n",
      "torch.Size([7])\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(net2.layers[1].dvdt))\n",
    "print(np.shape(net2.W[0]))\n",
    "print(np.shape(net2.layers[0].e))\n",
    "print(np.shape(net2.M[0]))\n",
    "print(np.shape(net2.layers[1].v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7])\n",
      "torch.Size([7, 8])\n",
      "torch.Size([8])\n",
      "torch.Size([8, 7])\n",
      "torch.Size([7])\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(net.layers[1].dvdt))\n",
    "print(np.shape(net.W[0]))\n",
    "print(np.shape(net.layers[0].e))\n",
    "print(np.shape(net.M[0]))\n",
    "print(np.shape(net.layers[1].v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 664,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.5916, -0.5986, -0.8456, -0.7550,  0.8802, -0.8882,  0.5235,\n",
      "         1.1115])\n",
      "tensor([ 1., -1., -1., -1.,  1., -1.,  1.,  1.])\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.randint(len(test[1]))\n",
    "xx = net.Generate(10., test[1][idx])\n",
    "print(xx)\n",
    "print(test[0][idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 663,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "idx = 2\n",
    "print(np.round(x_true[idx:idx+5],0)-np.round(x_est[idx:idx+5],0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1. -1. -1. -1.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 0.9986,  0.9992, -0.9991,  0.9972], device='cuda:0')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = 58\n",
    "print(train[1][idx])\n",
    "net.Predict(10., train[0][idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "widgets": {
   "state": {
    "8136bdaf4a3a4a7a8ebc121258db3244": {
     "views": [
      {
       "cell_index": 7
      }
     ]
    },
    "df58eb1fe74745dc82a439a5c68f9390": {
     "views": [
      {
       "cell_index": 10
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
